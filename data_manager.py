import os
import pandas as pd
import tushare as ts
import time
from datetime import datetime, timedelta
import sys
import numpy as np
from db_manager import db
import db_config

TUSHARE_API = None
TUSHARE_API_TOKEN = '36838688c6455de2e3affca37060648de15b94b9707a43bb05a38312'
DATA_START_DATE = '20240101'

SECTOR_STOCK_MAP = {
    'MARKET_PROXY':['601318','600519','300750','300308','600036','601899','300502','000333','300274','601166','600900'], # ‰∏≠ÂõΩÂπ≥ÂÆâ, Ë¥µÂ∑ûËåÖÂè∞, ÂÆÅÂæ∑Êó∂‰ª£, ‰∏≠ÈôÖÊó≠Âàõ, ÊãõÂïÜÈì∂Ë°å, Á¥´ÈáëÁüø‰∏ö, Êñ∞ÊòìÁõõ, ÁæéÁöÑÈõÜÂõ¢, Èò≥ÂÖâÁîµÊ∫ê, ÂÖ¥‰∏öÈì∂Ë°åÔºåÈïøÊ±üÁîµÂäõ
    'Èì∂Ë°å': ['601398','601939','601288'], # Â∑•ÂïÜÈì∂Ë°å, Âª∫ËÆæÈì∂Ë°å, ÂÜú‰∏öÈì∂Ë°å     
    'ÈùûÈì∂ÈáëËûç':['600030','601318','601628','000750','000776','002736','002670'], # ‰∏≠‰ø°ËØÅÂà∏, ‰∏≠ÂõΩÂπ≥ÂÆâ, ‰∏≠ÂõΩ‰∫∫ÂØø, ÂõΩÊµ∑ËØÅÂà∏, ÂπøÂèëËØÅÂà∏, ÂõΩ‰ø°ËØÅÂà∏
    'ÂçäÂØº‰Ωì': ['688981','688041','688256','002371','688347','001309','002049','603986'], # ‰∏≠ËäØÂõΩÈôÖ, Êµ∑ÂÖâ‰ø°ÊÅØ, ÂØíÊ≠¶Á∫™ÔºåÂåóÊñπÂçéÂàõÔºåÂçéËôπÂÖ¨Âè∏ÔºåÂæ∑ÊòéÂà©ÔºåÁ¥´ÂÖâÂõΩÂæÆÔºåÂÖÜÊòìÂàõÊñ∞
    'ËΩØ‰ª∂': ['688111','002230','600588','300033','601360','300339','600570'],   # ÈáëÂ±±ÂäûÂÖ¨, ÁßëÂ§ßËÆØÈ£û, Áî®ÂèãÁΩëÁªú, ÂêåËä±È°∫Ôºå‰∏âÂÖ≠Èõ∂ÔºåÊ∂¶ÂíåËΩØ‰ª∂ÔºåÊÅíÁîüÁîµÂ≠ê
    'ÂÖâÊ®°Âùó‰∏≠Ê∏∏': ['300308','300394','002281','603083','300620','300548'], # ‰∏≠ÈôÖÊó≠Âàõ, Â§©Â≠öÈÄöËÆØ, ÂÖâËøÖÁßëÊäÄÔºåÂâëÊ°•ÁßëÊäÄÔºåÂÖâÂ∫ìÁßëÊäÄÔºåÈïøÂÖ¥ÂçöÂàõ
    'Ê∂≤ÂÜ∑':['002837','300499','301018','603019','000977'],# Ëã±Áª¥ÂÖãÔºåÈ´òÊæúËÇ°‰ªΩÔºåÁî≥Ëè±ÁéØÂ¢ÉÔºå‰∏≠ÁßëÊõôÂÖâÔºåÊµ™ÊΩÆ‰ø°ÊÅØ
    'ÂÜõÂ∑•ÁîµÂ≠ê': ['600760','002414','600562','002179','688002','600990'],  # ‰∏≠Ëà™Ê≤àÈ£û, È´òÂæ∑Á∫¢Â§ñ, ÂõΩÁùøÁßëÊäÄ, ‰∏≠Ëà™ÂÖâÁîµ, ÁùøÂàõÂæÆÁ∫≥, ÂõõÂàõÁîµÂ≠ê
    'È£éÁîµËÆæÂ§á':['002202','002531','002487','300443'], # ÈáëÈ£éÁßëÊäÄ, Â§©È°∫È£éËÉΩ, Â§ßÈáëÈáçÂ∑•,ÈáëÈõ∑ËÇ°‰ªΩ
    'ÂÆ∂Áî®ÁîµÂô®': ['000333','600690','000921','000651','002050','603486'], # ÁæéÁöÑÈõÜÂõ¢, Êµ∑Â∞îÊô∫ÂÆ∂, Êµ∑‰ø°ÂÆ∂Áîµ, Ê†ºÂäõÁîµÂô®, ‰∏âËä±Êô∫Êéß, ÁßëÊ≤ÉÊñØ
    'ÁîµÂäõ': ['600900','601985','600886','600905','600795','600157'], # ÈïøÊ±üÁîµÂäõ, ‰∏≠ÂõΩÊ†∏Áîµ, ÂõΩÊäïÁîµÂäõ, ‰∏âÂ≥°ËÉΩÊ∫ê, ÂõΩÁîµÁîµÂäõ, Ê∞∏Ê≥∞ËÉΩÊ∫ê
    'ÁôΩÈÖí': ['000568','000596', '600809','600519'],    # Ê≥∏Â∑ûËÄÅÁ™ñ, Âè§‰∫ïË¥°ÈÖí, Â±±Ë•øÊ±æÈÖíÔºåË¥µÂ∑ûËåÖÂè∞
    'ÁîµÁΩëËÆæÂ§á':['600406','002028','600089','601877','300274','600312','601179'], # ÂõΩÁîµÂçóÁëû, ÊÄùÊ∫êÁîµÊ∞î, ÁâπÂèòÁîµÂ∑•ÔºåÁîüÊÄÅÁîµÂô®ÔºåÈò≥ÂÖâÁîµÊ∫êÔºåÂπ≥È´òÁîµÊ∞îÔºå‰∏≠ÂõΩË•øÁîµ
    'ÁîµÊ±†': ['300014','002074','300750','688778','300450'],   # ‰∫øÁ∫¨ÈîÇËÉΩ, ÂõΩËΩ©È´òÁßë, ÂÆÅÂæ∑Êó∂‰ª£ÔºåÂé¶Èí®Êñ∞ËÉΩÔºåÂÖàÂØºÊô∫ËÉΩ
    'Êï¥ËΩ¶':['600104','601633','000625','601238','002594','600418'], # ‰∏äÊ±ΩÈõÜÂõ¢, ÈïøÂüéÊ±ΩËΩ¶, ÈïøÂÆâÊ±ΩËΩ¶ÔºåÂπøÊ±ΩÊ±ΩËΩ¶ÔºåÊØî‰∫öËø™ÔºåÊ±üÊ∑ÆÊ±ΩËΩ¶
    'ÊúâËâ≤ÈáëÂ±û':['000630','000878','601899','600362','601600','000426'], # ÈìúÈôµÊúâËâ≤, ‰∫ëÈìùËÇ°‰ªΩ, Á¥´ÈáëÁüø‰∏ö, Ê±üË•øÈìú‰∏ö, ‰∏≠ÂõΩÈìù‰∏öÔºåÂÖ¥‰∏öÈì∂Èî°
    'ËÉΩÊ∫ê':['601800','601857','601225','600028','600938','002353','600188'] # ‰∏≠ÂõΩ‰∫§Âª∫, ‰∏≠ÂõΩÁü≥Ê≤π, ÈôïË•øÁÖ§‰∏ö, ‰∏≠ÂõΩÁü≥Âåñ, ‰∏≠ÂõΩÊµ∑Ê≤π, Êù∞ÁëûËÇ°‰ªΩÔºåÂÖñÂàõËÉΩÊ∫ê
}


ALL_STOCK_TICKERS = sorted(list(set(ticker for stocks in SECTOR_STOCK_MAP.values() for ticker in stocks)))
REQUIRED_COLUMNS = ['Open', 'Close', 'High', 'Low', 'Volume']
MIN_HISTORY_DAYS = 100
VOL_ZSCORE_LOOKBACK = 100

def init_tushare(token=None):
    """Initializes the Tushare API."""
    global TUSHARE_API
    if token is None:
        token = TUSHARE_API_TOKEN
    
    if token == '36838688c6455de2e3affca37060648de15b94b9707a43bb05a38312':
        pass
    
    try:
        ts.set_token(token)
        TUSHARE_API = ts.pro_api()
        print("Tushare token set. (Test query skipped)")
        return True
    except Exception as e:
        print(f"Tushare initialization failed: {e}")
        return False

def get_tushare_ticker(ticker):
    """Converts 6-digit ticker to Tushare's format."""
    ticker = ticker.split('.')[0]
    
    if ticker.startswith(('43', '83', '87')):
        return f"{ticker}.BJ"
    elif ticker.startswith('6') or ticker.startswith('688'):
        return f"{ticker}.SH"
    elif ticker.startswith(('0', '2', '3')):
        return f"{ticker}.SZ"
    return ticker

def create_stock_basic_table():
    """Creates stock_basic table (only for SQLite)."""
    if db_config.USE_SQLITE:
        schema = """CREATE TABLE IF NOT EXISTS stock_basic (
            ts_code TEXT PRIMARY KEY,
            symbol TEXT,
            name TEXT,
            area TEXT,
            industry TEXT,
            market TEXT,
            list_date TEXT,
            last_updated TEXT
        )"""
        db.create_table_sqlite(schema)


def update_stock_basic_table():
    """Fetch all stock basic info from Tushare and update database."""
    global TUSHARE_API
    if TUSHARE_API is None:
        init_tushare()
    if TUSHARE_API is None:
        print("[data_manager] ‚ùå Cannot update stock_basic - Tushare not initialized")
        return False

    # Use Beijing time
    from zoneinfo import ZoneInfo
    CHINA_TZ = ZoneInfo("Asia/Shanghai")
    today_beijing = datetime.now(CHINA_TZ).date()
    today_str = today_beijing.strftime('%Y-%m-%d')

    create_stock_basic_table()
    
    try:
        # Check last update (compare Beijing dates)
        existing = db.read_table('stock_basic', columns='last_updated', order_by='-last_updated', limit=1)
        if not existing.empty:
            last_update = existing['last_updated'].iloc[0]
            last_update_date = datetime.strptime(last_update, '%Y-%m-%d').date()
            
            if last_update_date == today_beijing:
                print(f"[data_manager] ‚ÑπÔ∏è stock_basic already updated today (Beijing: {today_str})")
                return False
    except:
        pass
    
    print(f"[data_manager] üîÑ Fetching stock_basic from Tushare (Beijing date: {today_str})...")
    
    try:
        df = TUSHARE_API.stock_basic(fields='ts_code,symbol,name,area,industry,market,list_date')
        if df.empty:
            print("[data_manager] ‚ö†Ô∏è No data returned from stock_basic API")
            return False
        
        # Use Beijing date for last_updated
        df['last_updated'] = today_str
        
        # Clear old data
        db.delete_all_records('stock_basic')
        
        # Insert new data
        records = df.to_dict('records')
        db.insert_records('stock_basic', records, upsert=True)
        
        print(f"[data_manager] ‚úÖ Updated stock_basic table with {len(df)} stocks (Beijing: {today_str})")
        return True
    except Exception as e:
        print(f"[data_manager] ‚ùå Failed to fetch stock_basic: {e}")
        return False


def get_company_name_from_api(ticker):
    """Fallback: Fetch company name from Tushare stock_company API."""
    global TUSHARE_API
    if TUSHARE_API is None:
        init_tushare()
    if TUSHARE_API is None:
        print(f"[data_manager] ‚ùå Cannot fetch from API - Tushare not initialized")
        return None
    
    try:
        ts_code = get_tushare_ticker(ticker)
        time.sleep(0.31)
        df = TUSHARE_API.stock_company(ts_code=ts_code, fields='ts_code,com_name')
        if not df.empty:
            name = df.iloc[0]['com_name']
            print(f"[data_manager] ‚úÖ Fetched from API: {ticker} -> {name}")
            return name
        else:
            print(f"[data_manager] ‚ö†Ô∏è No company info found for {ticker}")
            return None
    except Exception as e:
        print(f"[data_manager] ‚ùå Failed to fetch from API for {ticker}: {e}")
        return None

def get_stock_name_from_db(ticker):
    """Get stock name from database stock_basic table."""
    ts_code = get_tushare_ticker(ticker)
    
    try:
        df = db.read_table('stock_basic', filters={'ts_code': ts_code}, columns='name')
        if not df.empty:
            return df.iloc[0]['name']
        else:
            print(f"[data_manager] ‚ö†Ô∏è Stock {ticker} not found in stock_basic table")
            return None
    except Exception as e:
        print(f"[data_manager] ‚ùå Error fetching stock name: {e}")
        return None

def ensure_stock_basic_updated():
    """Ensures stock_basic table is updated (max once per day). Call this at app startup."""
    try:
        update_stock_basic_table()
    except Exception as e:
        print(f"[data_manager] ‚ö†Ô∏è Could not update stock_basic: {e}")

def create_history_table():
    """Creates search history table (only for SQLite)."""
    if db_config.USE_SQLITE:
        schema = """CREATE TABLE IF NOT EXISTS search_history (
            ticker TEXT PRIMARY KEY,
            timestamp TEXT,
            company_name TEXT
        )"""
        db.create_table_sqlite(schema)

def update_search_history(ticker):
    """Updates search history table with company name, keeps only last 10."""
    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    print(f"[data_manager] üîç Updating search history for {ticker}...")
    
    create_history_table()
    
    company_name = get_stock_name_from_db(ticker)
    if company_name is not None:
        print(f"[data_manager] üì¶ Found in stock_basic DB: {company_name}")
    else:
        print(f"[data_manager] ‚ö†Ô∏è Not in stock_basic DB, calling stock_company API...")
        company_name = get_company_name_from_api(ticker)
        if not company_name:
            company_name = ticker
            print(f"[data_manager] ‚ö†Ô∏è API also failed, using ticker: {ticker}")
    
    try:
        # Upsert record
        db.insert_records('search_history', [{
            'ticker': ticker,
            'timestamp': timestamp,
            'company_name': company_name
        }], upsert=True)
        
        # Keep only last 10
        all_history = db.read_table('search_history', columns='ticker,timestamp', order_by='-timestamp')
        if len(all_history) > 10:
            to_delete = all_history.iloc[10:]['ticker'].tolist()
            for t in to_delete:
                db.delete_records('search_history', {'ticker': t})
        
        print(f"[data_manager] ‚úÖ History saved: {ticker} - {company_name}")
        return company_name
    except Exception as e:
        print(f"[data_manager] ‚ùå Error updating search history: {e}")
        return ticker

def get_search_history(limit=10):
    """Returns list of last 10 searched stocks with display names."""
    try:
        create_history_table()
        df = db.read_table('search_history', columns='ticker,company_name,timestamp', order_by='-timestamp', limit=limit)
        
        if df.empty:
            return []
        
        results = []
        for _, row in df.iterrows():
            ticker = row['ticker']
            company_name = row.get('company_name', ticker)
            results.append({
                'ticker': ticker,
                'display': f"{company_name} ({ticker})"
            })
        return results
    except Exception as e:
        print(f"[data_manager] ‚ùå Error fetching search history: {e}")
        return []

def create_table(ticker):
    """Creates a table for a specific stock (only for SQLite)."""
    if db_config.USE_SQLITE:
        schema = f"""CREATE TABLE IF NOT EXISTS "{ticker}" (
            Date TEXT PRIMARY KEY,
            Open REAL,
            High REAL,
            Low REAL,
            Close REAL,
            Volume REAL
        )"""
        db.create_table_sqlite(schema)

def create_ppi_table(sector_name):
    """Creates a table for a specific aggregated PPI (only for SQLite)."""
    if db_config.USE_SQLITE:
        table_name = f"PPI_{sector_name}"
        schema = f"""CREATE TABLE IF NOT EXISTS "{table_name}" (
            Date TEXT PRIMARY KEY,
            Open REAL,
            High REAL,
            Low REAL,
            Close REAL,
            Norm_Vol_Metric REAL
        )"""
        db.create_table_sqlite(schema)

def get_last_date_in_db(ticker):
    """Finds the last date of data stored for a specific ticker."""
    try:
        if not db.table_exists(ticker):
            return None
        
        df = db.read_table(ticker, columns='Date', order_by='-Date', limit=1)
        if not df.empty:
            return df.iloc[0]['Date']
        return None
    except:
        return None

def insert_data(ticker, df):
    """Inserts a DataFrame into the stock's table."""
    df_to_insert = df[['Open', 'High', 'Low', 'Close', 'Volume']].copy()
    df_to_insert.index.name = 'Date'
    df_to_insert.reset_index(inplace=True)
    
    # Normalize Date to string format YYYY-MM-DD
    df_to_insert['Date'] = pd.to_datetime(df_to_insert['Date']).dt.strftime('%Y-%m-%d')

    records = df_to_insert.to_dict('records')
    db.insert_records(ticker, records, upsert=True)

def fetch_stock_data_robust(ticker, start_date, end_date):
    """Fetches and reconstructs hfq (forward-adjusted) data using Tushare Pro."""
    global TUSHARE_API
    ts_code = get_tushare_ticker(ticker)
    
    try:
        df = TUSHARE_API.daily(ts_code=ts_code, start_date=start_date, end_date=end_date,
                               fields='trade_date,open,close,high,low,vol')
        if df.empty:
            return pd.DataFrame()
        
        adj_factor_df = TUSHARE_API.adj_factor(ts_code=ts_code, start_date=start_date, end_date=end_date,
                                               fields='trade_date,adj_factor')
        if adj_factor_df.empty:
            raise ValueError(f"Tushare 'adj_factor' returned empty for {ts_code}.")
        
        df = pd.merge(df, adj_factor_df, on='trade_date', how='left')
        df['adj_factor'] = df['adj_factor'].fillna(method='ffill')
        df.dropna(inplace=True)
        
        last_factor = df['adj_factor'].iloc[0]
        df['adj_factor_hfq'] = df['adj_factor'] / last_factor
        
        df['Open'] = df['open'] * df['adj_factor_hfq']
        df['Close'] = df['close'] * df['adj_factor_hfq']
        df['High'] = df['high'] * df['adj_factor_hfq']
        df['Low'] = df['low'] * df['adj_factor_hfq']
        df['Volume'] = df['vol']
        df['Date'] = pd.to_datetime(df['trade_date'])
        
        df.set_index('Date', inplace=True)
        df.sort_index(inplace=True)
        df = df[REQUIRED_COLUMNS]
        
        return df
    except Exception as e:
        error_message = f"'{str(e)}'"
        print(f" - FATAL ERROR: Fetch failed for {ticker}. Error: {error_message}")
        return None

def ensure_data_in_db(start_date, end_date):
    """Main data function. Ensures all data from start_date to end_date is in the DB."""
    for ticker in ALL_STOCK_TICKERS:
        create_table(ticker)
        last_db_date_str = get_last_date_in_db(ticker)
        fetch_start_date = start_date
        
        if last_db_date_str:
            try:
                last_db_date_dt = pd.to_datetime(last_db_date_str).to_pydatetime()
            except Exception:
                print(f"Warning: Could not parse date {last_db_date_str} for {ticker}. Refetching all.")
                last_db_date_str = None
            
            if last_db_date_str:
                last_db_date_fmt = last_db_date_dt.strftime('%Y%m%d')
                if last_db_date_fmt >= end_date:
                    continue
                fetch_start_date = (last_db_date_dt + timedelta(days=1)).strftime('%Y%m%d')
        
        print(f" -> Fetching new data for {ticker} from {fetch_start_date} to {end_date}...")
        new_data_df = fetch_stock_data_robust(ticker, fetch_start_date, end_date)
        
        if new_data_df is not None and not new_data_df.empty:
            if last_db_date_str:
                new_data_df = new_data_df[new_data_df.index > last_db_date_str]
            
            if not new_data_df.empty:
                insert_data(ticker, new_data_df)
                print(f" - Successfully updated {ticker} with {len(new_data_df)} new rows.")
            else:
                print(f" - No new data to add for {ticker}.")
        elif new_data_df is not None and new_data_df.empty:
            print(f" - No new data found for {ticker} in this period (weekend/holiday).")
        else:
            print(f" - FAILED to fetch new data for {ticker}.")
        
        time.sleep(1.0)

def get_all_stock_data_from_db():
    """Loads all stock data from the DB into a dictionary of DataFrames."""
    all_stock_data = {}
    
    for ticker in ALL_STOCK_TICKERS:
        try:
            if not db.table_exists(ticker):
                continue
            
            df = db.read_table(ticker)
            if not df.empty:
                df['Date'] = pd.to_datetime(df['Date'], format='mixed', errors='coerce')
                df = df.set_index('Date').sort_index()
                
                missing_cols_db = [col for col in REQUIRED_COLUMNS if col not in df.columns]
                if missing_cols_db:
                    print(f"!! WARNING: Data for {ticker} in DB is missing columns: {missing_cols_db}.")
                else:
                    df['Vol_Mean'] = df['Volume'].rolling(window=VOL_ZSCORE_LOOKBACK, min_periods=20).mean()
                    df['Vol_Std'] = df['Volume'].rolling(window=VOL_ZSCORE_LOOKBACK, min_periods=20).std()
                    df['Norm_Vol_Metric'] = (df['Volume'] - df['Vol_Mean']) / df['Vol_Std']
                    all_stock_data[ticker] = df
        except Exception as e:
            print(f"Failed to load data for {ticker} from DB: {e}")
    
    return all_stock_data

def get_single_stock_data_from_db(ticker):
    """Loads a single stock's data from the DB."""
    try:
        if not db.table_exists(ticker):
            return None
        
        df = db.read_table(ticker)
        if not df.empty:
            df['Date'] = pd.to_datetime(df['Date'], format='mixed', errors='coerce')
            df = df.set_index('Date').sort_index()
            
            missing_cols_db = [col for col in REQUIRED_COLUMNS if col not in df.columns]
            if missing_cols_db:
                print(f"!! WARNING: Data for {ticker} in DB is missing columns: {missing_cols_db}.")
                return None
            
            df['Vol_Mean_100d'] = df['Volume'].rolling(window=VOL_ZSCORE_LOOKBACK, min_periods=20).mean()
            df['Vol_Std_100d'] = df['Volume'].rolling(window=VOL_ZSCORE_LOOKBACK, min_periods=20).std()
            df['Volume_ZScore'] = (df['Volume'] - df['Vol_Mean_100d']) / df['Vol_Std_100d']
            return df
        else:
            return None
    except Exception as e:
        print(f"Failed to load data for {ticker} from DB: {e}")
        return None

def get_single_stock_data(ticker, use_data_start_date: bool = True, lookback_years: int = 3):
    """Unified accessor for single-stock analysis."""
    if ticker not in ALL_STOCK_TICKERS:
        print(f"Warning: Ticker {ticker} not in the defined SECTOR_STOCK_MAP.")
    
    # Step 1: Try DB first
    df_db = get_single_stock_data_from_db(ticker)
    if df_db is not None and not df_db.empty:
        print(f"[data_manager] Loaded {ticker} from DB")
        return df_db
    
    # Step 2: Fetch from Tushare
    global TUSHARE_API
    if TUSHARE_API is None:
        ok = init_tushare()
        if not ok:
            print("[data_manager] Tushare initialization failed inside get_single_stock_data.")
            return None
    
    if use_data_start_date:
        start_str = DATA_START_DATE
    else:
        end_dt = datetime.today()
        start_dt = end_dt - timedelta(days=365 * lookback_years)
        start_str = start_dt.strftime("%Y%m%d")
    
    end_str = datetime.today().strftime("%Y%m%d")
    
    print(f"[data_manager] {ticker} not found in DB, fetching from Tushare ({start_str} ‚Üí {end_str})...")
    df_ts = fetch_stock_data_robust(ticker, start_str, end_str)
    
    if df_ts is None or df_ts.empty:
        print(f"[data_manager] Tushare fetch failed or returned empty for {ticker}.")
        return None
    
    # Step 3: Save to DB
    try:
        create_table(ticker)
        insert_data(ticker, df_ts)
        print(f"[data_manager] Saved {ticker} from Tushare into DB ({len(df_ts)} rows).")
    except Exception as e:
        print(f"[data_manager] Failed to save {ticker} Tushare data to DB: {e}")
    
    # Step 4: Reload from DB to get calculated metrics
    df_final = get_single_stock_data_from_db(ticker)
    if df_final is None or df_final.empty:
        return df_ts
    return df_final

def aggregate_ppi_data(all_stock_data):
    """Aggregates individual stock data into sector-level Proxy Portfolio Indexes (PPIs)."""
    print("\n--- Aggregating Stock Data into Sector PPIs (V3.6 Logic) ---")
    
    all_sector_ppi_data = {}
    
    for sector, stock_list in SECTOR_STOCK_MAP.items():
        named_dfs = []
        for ticker in stock_list:
            if ticker in all_stock_data:
                temp_df = all_stock_data[ticker].copy()
                is_halted = (temp_df['Volume'] == 0)
                temp_df[is_halted] = np.nan
                temp_df.name = ticker
                named_dfs.append(temp_df)
        
        if not named_dfs: continue
        
        aligned_df = pd.concat(named_dfs, axis=1, keys=[df.name for df in named_dfs], join='outer')
        
        valid_tickers = [t for t in stock_list if (t, 'Close') in aligned_df.columns]
        if not valid_tickers: continue
        
        open_prices = aligned_df.xs('Open', level=1, axis=1)[valid_tickers]
        high_prices = aligned_df.xs('High', level=1, axis=1)[valid_tickers]
        low_prices = aligned_df.xs('Low', level=1, axis=1)[valid_tickers]
        close_prices = aligned_df.xs('Close', level=1, axis=1)[valid_tickers]
        
        prev_close_prices = close_prices.shift(1)
        
        ret_open = (open_prices / prev_close_prices - 1).mean(axis=1)
        ret_high = (high_prices / prev_close_prices - 1).mean(axis=1)
        ret_low = (low_prices / prev_close_prices - 1).mean(axis=1)
        ret_close = (close_prices / prev_close_prices - 1).mean(axis=1)
        
        norm_vol_metric = aligned_df.xs('Norm_Vol_Metric', level=1, axis=1)[valid_tickers]
        
        ppi_df = pd.DataFrame(index=aligned_df.index)
        ppi_df['Close'] = 100 * (1 + ret_close.fillna(0)).cumprod()
        ppi_df['Open'] = ppi_df['Close'].shift(1) * (1 + ret_open)
        ppi_df['High'] = ppi_df['Close'].shift(1) * (1 + ret_high)
        ppi_df['Low'] = ppi_df['Close'].shift(1) * (1 + ret_low)
        ppi_df['Norm_Vol_Metric'] = norm_vol_metric.mean(axis=1)
        
        ppi_df.dropna(inplace=True)
        
        if len(ppi_df) >= MIN_HISTORY_DAYS:
            all_sector_ppi_data[sector] = ppi_df
    
    print("--- PPI Aggregation Complete ---")
    return all_sector_ppi_data

def save_ppi_data_to_db(all_ppi_data):
    """Saves the aggregated PPI DataFrames into tables in the DB."""
    print(f"\n--- Saving {len(all_ppi_data)} PPIs to database ---")
    
    for sector_name, ppi_df in all_ppi_data.items():
        table_name = f"PPI_{sector_name}"
        create_ppi_table(sector_name)
        
        df_to_insert = ppi_df.copy()
        df_to_insert = df_to_insert[['Open', 'High', 'Low', 'Close', 'Norm_Vol_Metric']]
        df_to_insert.index.name = 'Date'
        df_to_insert.reset_index(inplace=True)
        
        records = df_to_insert.to_dict('records')
        
        # Clear old data first
        try:
            existing = db.read_table(table_name, columns='Date')
            if not existing.empty:
                for _, row in existing.iterrows():
                    db.delete_records(table_name, {'Date': row['Date']})
        except:
            pass
        
        # Insert new data
        db.insert_records(table_name, records, upsert=True)
        print(f" - Successfully saved '{table_name}' with {len(df_to_insert)} rows.")
    
    print("--- PPI Database Save Complete ---")

def load_ppi_data_from_db():
    """Loads all 'PPI_' tables from the DB into a dictionary of DataFrames."""
    print("\n--- Loading Pre-Calculated PPIs from Database ---")
    
    all_ppi_data = {}
    
    for sector_name in SECTOR_STOCK_MAP.keys():
        table_name = f"PPI_{sector_name}"
        try:
            if not db.table_exists(table_name):
                continue
            
            df = db.read_table(table_name)
            if not df.empty:
                df['Date'] = pd.to_datetime(df['Date'], format='mixed', errors='coerce')
                df = df.set_index('Date').sort_index()
                df.rename(columns={'Norm_Vol_Metric': 'Volume_Metric'}, inplace=True)
                all_ppi_data[sector_name] = df
        except Exception as e:
            print(f"Failed to load data for {table_name} from DB: {e}")
    
    if not all_ppi_data:
        print("!! ERROR: No PPI tables found in database.")
        print("!! Please run 'main.py' first to build the PPI tables.")
        return None
    
    print(f"--- Successfully loaded {len(all_ppi_data)} PPIs ---")
    return all_ppi_data

def create_signals_tables():
    """Creates signals cache tables (for Today's Alerts page)."""
    if db_config.USE_SQLITE:
        schema1 = """CREATE TABLE IF NOT EXISTS daily_signals (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            scan_date TEXT,
            type TEXT,
            ticker TEXT,
            name TEXT,
            signals TEXT,
            signal_count INTEGER,
            price REAL,
            rsi REAL,
            adx REAL,
            macd REAL,
            volume REAL,
            created_at TEXT
        )"""
        db.create_table_sqlite(schema1)
        
        try:
            db.execute_raw_sql("""CREATE INDEX IF NOT EXISTS idx_signals_scan_date 
                                 ON daily_signals(scan_date)""")
        except:
            pass
        
        schema2 = """CREATE TABLE IF NOT EXISTS signals_scan_metadata (
            scan_date TEXT PRIMARY KEY,
            total_stocks_scanned INTEGER,
            opportunities_found INTEGER,
            alerts_found INTEGER,
            scan_duration_seconds REAL,
            created_at TEXT
        )"""
        db.create_table_sqlite(schema2)
    # else: Supabase tables already exist from migration script

def get_cached_signals(scan_date):
    """Get cached signals for a specific date. Returns DataFrame if found, None otherwise."""
    try:
        # Check if data exists for this date
        check_df = db.read_table('daily_signals', filters={'scan_date': scan_date}, columns='*', limit=1)
        
        if check_df.empty:
            return None
        
        # Load cached data
        df = db.read_table('daily_signals', filters={'scan_date': scan_date}, columns='*', order_by='type, -signal_count')
        
        # Drop id and created_at columns
        df = df.drop(columns=['id', 'scan_date', 'created_at'], errors='ignore')
        
        # Rename columns to match expected format
        df = df.rename(columns={
            'type': 'Type',
            'ticker': 'Ticker',
            'name': 'Name',
            'signals': 'Signals',
            'signal_count': 'Signal_Count',
            'price': 'Price',
            'rsi': 'RSI',
            'adx': 'ADX',
            'macd': 'MACD',
            'volume': 'Volume'
        })
        
        return df
    except Exception as e:
        print(f"Error loading cached signals: {e}")
        return None

def save_signals_to_cache(df, scan_date, scan_duration):
    """Save scanned signals to database cache."""
    if df is None or df.empty:
        return False
    
    try:
        # Delete old data for this date (in case of rescan)
        db.delete_records('daily_signals', {'scan_date': scan_date})
        db.delete_records('signals_scan_metadata', {'scan_date': scan_date})
        
        # Prepare data for insertion
        df_to_save = df.copy()
        df_to_save['scan_date'] = scan_date
        df_to_save['created_at'] = datetime.now().isoformat()
        
        # Rename columns to match database schema
        df_to_save = df_to_save.rename(columns={
            'Type': 'type',
            'Ticker': 'ticker',
            'Name': 'name',
            'Signals': 'signals',
            'Signal_Count': 'signal_count',
            'Price': 'price',
            'RSI': 'rsi',
            'ADX': 'adx',
            'MACD': 'macd',
            'Volume': 'volume'
        })
        
        # Save signals
        records = df_to_save.to_dict('records')
        db.insert_records('daily_signals', records, upsert=True)
        
        # Save metadata
        opportunities = len(df[df['Type'] == 'üöÄ Opportunity'])
        alerts = len(df[df['Type'] == '‚ö†Ô∏è Alert'])
        total_stocks = len(ALL_STOCK_TICKERS)
        
        metadata = {
            'scan_date': scan_date,
            'total_stocks_scanned': total_stocks,
            'opportunities_found': opportunities,
            'alerts_found': alerts,
            'scan_duration_seconds': scan_duration,
            'created_at': datetime.now().isoformat()
        }
        
        db.insert_records('signals_scan_metadata', [metadata], upsert=True)
        
        return True
    except Exception as e:
        print(f"Error saving signals to cache: {e}")
        return False

def get_scan_metadata(scan_date):
    """Get metadata about the last scan."""
    try:
        df = db.read_table('signals_scan_metadata', filters={'scan_date': scan_date})
        
        if df.empty:
            return None
        
        row = df.iloc[0]
        return {
            'scan_date': row['scan_date'],
            'total_stocks_scanned': row['total_stocks_scanned'],
            'opportunities_found': row['opportunities_found'],
            'alerts_found': row['alerts_found'],
            'scan_duration_seconds': row['scan_duration_seconds'],
            'created_at': row['created_at']
        }
    except:
        return None
